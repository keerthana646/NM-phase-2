# -*- coding: utf-8 -*-
"""keerthana phase 2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12jjBgzB6VvsGeBPqfFoIk6x0CDdadB2Z
"""

# -*- coding: utf-8 -*-
"""Untitled8.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1glimb4f9Xsw75sRx4sd0U8pmxseSzvJt
"""

# ===============================
# ðŸ“Œ FAKE NEWS DETECTION WITH NLP
# ===============================
# Project: Exposing the Truth with Advanced Fake News Detection Powered by NLP

# STEP 1: Import Required Libraries
import pandas as pd
import numpy as np
import re
import string
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from wordcloud import WordCloud

nltk.download('stopwords')
nltk.download('wordnet')

# STEP 2: Upload Dataset (Upload from Colab interface)
from google.colab import files
uploaded = files.upload()

# Assuming the uploaded file is named 'fake_news_dataset.csv'
df = pd.read_csv('fake_news_dataset.csv')

# STEP 3: Basic EDA
print(df.head())
print(df['label'].value_counts())
sns.countplot(x='label', data=df)
plt.title("Real vs Fake News Distribution")
plt.show()

# STEP 4: Text Cleaning
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

def clean_text(text):
    text = str(text).lower()
    text = re.sub(r'<.*?>', '', text)
    text = re.sub(r'http\S+', '', text)
    text = re.sub(r'[^a-z\s]', '', text)
    words = text.split()
    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]
    return ' '.join(words)

df['clean_text'] = df['text'].apply(clean_text)

# STEP 5: Word Clouds
fake_words = ' '.join(df[df['label'] == 0]['clean_text'])
real_words = ' '.join(df[df['label'] == 1]['clean_text'])

plt.figure(figsize=(14,6))
plt.subplot(1, 2, 1)
plt.imshow(WordCloud(width=600, height=400).generate(fake_words), interpolation='bilinear')
plt.axis('off')
plt.title("Fake News Word Cloud")

plt.subplot(1, 2, 2)
plt.imshow(WordCloud(width=600, height=400).generate(real_words), interpolation='bilinear')
plt.axis('off')
plt.title("Real News Word Cloud")
plt.show()

# STEP 6: Train/Test Split
X = df['clean_text']
y = df['label']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# STEP 7: TF-IDF Vectorization
tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1,2))
X_train_tfidf = tfidf.fit_transform(X_train)
X_test_tfidf = tfidf.transform(X_test)

# STEP 8: Train the Model
model = LogisticRegression(max_iter=1000)
model.fit(X_train_tfidf, y_train)

# STEP 9: Predictions & Evaluation
y_pred = model.predict(X_test_tfidf)

print("Accuracy Score:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

# Confusion Matrix Heatmap
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Fake', 'Real'], yticklabels=['Fake', 'Real'])
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
plt.show()

# STEP 10: Feature Importance (Top Words)
feature_names = tfidf.get_feature_names_out()
coefs = model.coef_[0]

top_fake = np.argsort(coefs)[:20]
top_real = np.argsort(coefs)[-20:]

plt.figure(figsize=(14,6))
plt.subplot(1, 2, 1)
plt.barh(range(20), coefs[top_fake])
plt.yticks(range(20), [feature_names[i] for i in top_fake])
plt.title("Top Words for Fake News")

plt.subplot(1, 2, 2)
plt.barh(range(20), coefs[top_real])
plt.yticks(range(20), [feature_names[i] for i in top_real])
plt.title("Top Words for Real News")

plt.tight_layout()
plt.show()